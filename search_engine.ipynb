{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sys import stdout\n",
    "\n",
    "search_keys = [\"Willis\", \"Sycamore\", \"Discovery Park\", \"Welch\", \"Union\", \"General Academic Building\", \"Gateway center\", \"Research\", \"Computer Science\", \"Data Engineering\"]\n",
    "\n",
    "url_list = []\n",
    "for search in search_keys:\n",
    "    url = 'https://www.unt.edu/search-results?search='+search.replace(' ', '+')+'&sa=Search'\n",
    "    url_list.append(url)\n",
    "\n",
    "contents = []\n",
    "\n",
    "for url in url_list:\n",
    "    driver = uc.Chrome(use_subprocess=True,)\n",
    "    driver.get(url)    \n",
    "\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, \"//*[@class='gsc-wrapper']\")))\n",
    "\n",
    "    for index in range(10):\n",
    "        try:\n",
    "            index += 1 \n",
    "            WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, \"//*[@class='gsc-cursor']/div[\"+str(index)+\"]\"))).click()\n",
    "\n",
    "            time.sleep(1)\n",
    "            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, \"//*[@class='gsc-wrapper']\")))\n",
    "            elements = driver.find_elements(By.XPATH, \"//*[@class='gsc-thumbnail-inside']\")\n",
    "            for element in elements:\n",
    "                contents.append(element.text)\n",
    "        except:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First: \n",
    "    Query sets:\n",
    "        1. Willis\n",
    "        2. Sycamore\n",
    "        3. Discovery Park\n",
    "        4. Welch\n",
    "        5. Union\n",
    "        6. General Academic Building\n",
    "        7. Gateway center\n",
    "        8. Research\n",
    "        9. Computer Science\n",
    "        10. Data Engineering\n",
    "Run the query on the original unt.edu and scrape the search results from UNT.edu site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second: Save the scarpped results to csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "texts = []\n",
    "for content in  contents:\n",
    "    if not content == '':\n",
    "        texts.append(content)\n",
    "df = pd.DataFrame(\n",
    "    {'texts': texts})\n",
    "df.to_csv('resources/text.csv', index=True, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third: Create a dictionary of words (Indexing) for vector space retrieval model for the search  and then create a search engine with vector space retrieval model for the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing....\n",
      "0            Willis Library - University Libraries - UNT\n",
      "1      Reserving a Study Space - University Libraries...\n",
      "2                                          UNT Libraries\n",
      "3                    Spaces - University Libraries - UNT\n",
      "4                           Willis Library: Fourth Floor\n",
      "                             ...                        \n",
      "548                               Mechanical Engineering\n",
      "549    Dataflow based Near Data Computing Achieves Ex...\n",
      "550    Dataflow based Near-Data Processing using Coar...\n",
      "551    * The Program Inventory displays the minimum n...\n",
      "552    An Industrial Case Study About Test Failure Pr...\n",
      "Name: texts, Length: 553, dtype: object\n",
      "Index done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "import pandas\n",
    "from collections import Counter\n",
    "from utils import textprocessing, helpers\n",
    "''' Index data '''\n",
    "\n",
    "print('Indexing....')\n",
    "\n",
    "resources_path = os.path.join(os.getcwd(), 'resources')\n",
    "data_path = os.path.join(os.getcwd(), 'data')\n",
    "\n",
    "if not os.path.isdir(resources_path):\n",
    "    print('ERROR: The {} is not a directory or does not exist'.format(\n",
    "        resources_path))\n",
    "    sys.exit(1)\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    os.mkdir(data_path)\n",
    "\n",
    "# Get dataset path and stopwords file\n",
    "dataset_path = os.path.join(resources_path, 'text.csv')\n",
    "stopwords_file = os.path.join(resources_path, 'stopwords_en.txt')\n",
    "\n",
    "# Get stopwords set\n",
    "stopwords = helpers.get_stopwords(stopwords_file)\n",
    "\n",
    "df = pandas.read_csv(dataset_path)\n",
    "print(df.get('texts'))\n",
    "docs = list(df.get('texts'))\n",
    "\n",
    "\n",
    "corpus = []\n",
    "for doc in docs:    \n",
    "    text = doc\n",
    "    words = textprocessing.preprocess_text(text, stopwords)\n",
    "    bag_of_words = Counter(words)\n",
    "    corpus.append(bag_of_words)\n",
    "\n",
    "idf = helpers.compute_idf(corpus)\n",
    "for doc in corpus:\n",
    "    helpers.compute_weights(idf, doc)\n",
    "    helpers.normalize(doc)\n",
    "\n",
    "inverted_index = helpers.build_inverted_index(idf, corpus)\n",
    "\n",
    "docs_file = os.path.join(data_path, 'docs.pickle')\n",
    "inverted_index_file = os.path.join(data_path, 'inverted_index.pickle')\n",
    "dictionary_file = os.path.join(data_path, 'dictionary.txt')\n",
    "\n",
    "# Serialize data\n",
    "with open(docs_file, 'wb') as f:\n",
    "    pickle.dump(docs, f)\n",
    "\n",
    "with open(inverted_index_file, 'wb') as f:\n",
    "    pickle.dump(inverted_index, f)\n",
    "\n",
    "with open(dictionary_file, 'w') as f:\n",
    "    for word in idf.keys():\n",
    "        f.write(word + '\\n')\n",
    "\n",
    "print('Index done.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally Run query of same search key on created engine with vector space retrieval model for the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Results query: (Computer Science)------ \n",
      "1. Computer Science & Engineering - 0.8810942888417447\n",
      "2. Department of Computer Science and Engineering - 0.7547141503480617\n",
      "3. Department of Computer Science and Engineering - University of ... - 0.7304441293993396\n",
      "4. Contact Us | Computer Science and Engineering - 0.6908853157399746\n",
      "5. Contact Us | Computer Science and Engineering - 0.6908853157399746\n",
      "6. Discovery Park B205 | Computing for Arts + Sciences - 0.6684611170637984\n",
      "7. GATE 141 | Computing for Arts + Sciences - 0.5357695888096836\n",
      "8. Departmental Staff | Computer Science and Engineering - 0.520615437958739\n",
      "9. Computer Labs - University Libraries - UNT - 0.42836244440554433\n",
      "10. Computer Labs - University Libraries - UNT - 0.42836244440554433\n",
      "11. The Student Computer Lab at Discovery Park | College of Information - 0.3228691794000635\n",
      "12. Social Science | University of North Texas - 0.311183794929343\n",
      "13. Information Science | College of Information - 0.3041174301061119\n",
      "14. UNT Computer Labs | Academic Technologies - 0.3000981710226302\n",
      "15. Student Computer Lab | Information Technology Services - 0.28424818718601885\n",
      "16. Help Support Graduate Research at UNT! | College of Science - 0.28368126464820675\n",
      "17. Research BREAKS Session - High Performance Computing at UNT ... - 0.25052067625846447\n",
      "18. Research BREAKS Session - High Performance Computing at UNT ... - 0.25052067625846447\n",
      "19. Department of Political Science - University of North Texas - Acalog ... - 0.24233614983799942\n",
      "20. Life Sciences Complex | Facilities - 0.23874575127169695\n",
      "21. UNT engineering graduate student wins National Science ... - 0.2309384116179161\n",
      "22. New agreement leverages Texas Advanced Computing Center ... - 0.22640643868903837\n",
      "23. Lisa Welch | Department of Biological Sciences - 0.22567162087478906\n",
      "24. Get Advised Now! | College of Liberal Arts & Social Sciences - 0.22469477923126344\n",
      "25. Greenhouse at Discovery Park | Department of Biological Sciences - 0.21465586687300198\n",
      "26. Office of Student Advising | College of Liberal Arts & Social Sciences - 0.20666792928722613\n",
      "27. Advisor Contact Information | College of Liberal Arts & Social Sciences - 0.1862044385355273\n",
      "28. Dataflow based Near Data Computing Achieves Excellent Energy ... - 0.18169803677317609\n",
      "29. UNT's Academic Standards | College of Liberal Arts & Social Sciences - 0.17382505299589943\n",
      "30. Meet UNT Biological Sciences Lecturer Dr. Lisa Welch | College of ... - 0.16857358784943452\n",
      "31. UNT plant science discovery advances renewable energy research ... - 0.16738967918273906\n",
      "32. UNT's Science Education Research Lab wins water conservation ... - 0.15935309390399183\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "from utils import textprocessing\n",
    "from utils import helpers\n",
    "from collections import Counter\n",
    "''' Query '''\n",
    "\n",
    "docs_file = os.path.join(os.getcwd(), 'data', 'docs.pickle')\n",
    "inverted_index_file = os.path.join(\n",
    "    os.getcwd(), 'data', 'inverted_index.pickle')\n",
    "\n",
    "stopwords_file = os.path.join(os.getcwd(), 'resources', 'stopwords_en.txt')\n",
    "\n",
    "# Deserialize data\n",
    "with open(docs_file, 'rb') as f:\n",
    "    docs = pickle.load(f)\n",
    "with open(inverted_index_file, 'rb') as f:\n",
    "    inverted_index = pickle.load(f)\n",
    "\n",
    "stopwords = helpers.get_stopwords(stopwords_file)\n",
    "\n",
    "dictionary = set(inverted_index.keys())\n",
    "\n",
    "# Get query from command line\n",
    "query_input = input(\"Query: \")\n",
    "query = query_input\n",
    "# Preprocess query\n",
    "query = textprocessing.preprocess_text(query, stopwords)\n",
    "query = [word for word in query if word in dictionary]\n",
    "query = Counter(query)\n",
    "\n",
    "# Compute weights for words in query\n",
    "for word, value in query.items():\n",
    "    query[word] = inverted_index[word]['idf'] * (1 + math.log(value))\n",
    "\n",
    "helpers.normalize(query)\n",
    "\n",
    "scores = [[i, 0] for i in range(len(docs))]\n",
    "for word, value in query.items():\n",
    "    for doc in inverted_index[word]['postings_list']:\n",
    "        index, weight = doc\n",
    "        scores[index][1] += value * weight\n",
    "\n",
    "scores.sort(key=lambda doc: doc[1], reverse=True)\n",
    "\n",
    "print('----- Results query: ('+ query_input +')------ ')\n",
    "for index, score in enumerate(scores):\n",
    "    if score[1] == 0:\n",
    "        break\n",
    "    print('{}. {} - {}'.format(index + 1, docs[score[0]], score[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "61b6d9b733b87f2a24def837dfeb7441a71b46a3ec4671f28b3fbb0056422bb6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
